# High-Speed Train Control with Reinforcement Learning

This project implements deep reinforcement learning algorithms (A2C and SAC) for high-speed train operation control, featuring a comprehensive train dynamics simulation environment and intelligent control strategies.

# üìÅ Project Structure

- **`RL_A2C.py`**: Advantage Actor-Critic (A2C) algorithm implementation
- **`RL_SAC.py`**: Soft Actor-Critic (SAC) algorithm implementation  
- **`TrainEnv.py`**: High-speed train simulation environment
- **`TrainControl.py`**: Main control system integrating SAC with train environment
- **`MaxCapabilityCurve.py`**: Maximum capability curve generator for trains
- **`utils.py`**: Utility functions including replay buffer implementation
- **Excel files**: Train characteristics and railway line data

## üöÄ Quick Start

### Requirements

```bash
pip install torch numpy pandas gymnasium matplotlib scipy numba
```

### Environment Requirements

- Python 3.8+
- PyTorch 1.9+
- CUDA support (recommended for training)

### Usage

1. **Train the model**:
   ```python
   python TrainControl.py
   ```

2. **Test with pre-trained model**:
   ```python
   # Uncomment the test lines in TrainControl.py
   agent.load_model("pendulum_sac_train_final.pth")
   agent.test()
   ```

3. **Generate maximum capability curves**:
   ```python
   python MaxCapabilityCurve.py
   ```

# üß† Algorithm Overview

---

# PART 1: SAC Algorithm Pseudocode

## Initialization Phase

1. Initialize policy network (Actor) $\pi_{\theta}$ with parameters $\theta$
2. Initialize two Q-networks (Critics) $Q_{\phi_1}$, $Q_{\phi_2}$ with parameters $\phi_1,\phi_2$
3. Initialize two target Q-networks $Q_{\phi'_1}$, $Q_{\phi'_2}$ with same parameters: $\phi'_1 \leftarrow \phi_1$, $\phi'_2 \leftarrow \phi_2$
4. Initialize experience replay buffer $\mathcal{D}$
5. Initialize learnable log temperature parameter $\log \alpha$
6. Define target entropy $\mathcal{H}_{target}$ (typically -action_dimension)
7. Initialize total steps `total_steps = 0`

## Main Training Loop

**FOR** `episode` = 1 **TO** `max_episodes` **DO**:

1. Reset environment and get initial state $s$

2. **FOR** `step` = 1 **TO** `max_steps` **DO**:

    a. **IF** `total_steps` < `start_steps`:
    - Randomly sample action from action space: a = self.env.action_space.sample()

    b. **ELSE**:
    - Sample action from current policy: $a \sim \pi_{\theta}(\cdot|s)$ (via `select_action` function)

    c. Execute action $a$ in environment, obtain next state $s'$, reward $r$, and done flag $d$

    d. Store transition tuple $(s, a, r, s', d)$ in replay buffer $\mathcal{D}$

    e. Update current state: $s \leftarrow s'$

    f. `total_steps` $\leftarrow$ `total_steps` + 1

    g. **IF** `total_steps` > `start_steps`:

    1. Randomly sample a minibatch from $\mathcal{D}$ : $\{(s_j, a_j, r_j, s'_j, d_j)\}_{j=1}^{N}$
 
    2. **--- Update Critic Networks ---**

        i. **Compute target values y** (without gradients):

        - Sample next actions and log probabilities from current policy: 

        $$
        a'_j \sim \pi_{\theta}(\cdot|s'_j), \log\pi_{\theta}(a'_j|s'_j)
        $$

        - Compute Q-values for next states using **target Q-networks** with clipped double Q-learning:

        $$
        Q'_{target}(s'_j, a'_j) = \min(Q_{\phi'_1}(s'_j, a'_j), Q_{\phi'_2}(s'_j, a'_j))
        $$

        - Compute final target $y_j$ with entropy term:

        $$
        y_j = r_j + \gamma (1-d_j) (Q'_{target}(s'_j, a'_j) - \alpha \log\pi_{\theta}(a'_j|s'_j))
        $$

        ii. **Compute current Q-values** using experience buffer actions:

        $$
        Q_{\phi_1}(s_j, a_j), Q_{\phi_2}(s_j, a_j)
        $$

        iii. **Compute Critic loss** using Mean Squared Error (MSE):

        $$
        L_{\text{critic}} = \frac{1}{N}\sum_{j=1}^{N} \left( (Q_{\phi_1}(s_j, a_j) - y_j)^2 + (Q_{\phi_2}(s_j, a_j) - y_j)^2 \right)
        $$

        iv. **Update Critic parameters** via gradient descent on $L_{\text{critic}}$

    3. **--- Update Actor Network and Temperature $\alpha$ ---**

        i. **Compute Actor loss**:

        - Sample new actions from current policy for states $s_j$: $\tilde{a}_j \sim \pi_{\theta}(\cdot|s_j)$ with $\log\pi_{\theta}(\tilde{a}_j|s_j)$ (with gradients)

        - Compute Q-values for these actions using current **main Q-networks**:

        $$
        Q_{\pi}(s_j, \tilde{a}_j) = \min(Q_{\phi_1}(s_j, \tilde{a}_j), Q_{\phi_2}(s_j, \tilde{a}_j))
        $$

        - Actor loss (negative of soft Q-value to maximize):

        $$
        L_{\text{actor}} = \frac{1}{N}\sum_{j=1}^{N} (\alpha \log\pi_{\theta}(\tilde{a}_j|s_j) - Q_{\pi}(s_j, \tilde{a}_j))
        $$

        ii. **Update Actor parameters** via gradient descent on $L_{\text{actor}}$

        iii. **Compute Alpha loss**:

        $$
        L_{\alpha} = \frac{1}{N}\sum_{j=1}^{N} (-\log\alpha (\log\pi_{\theta}(\tilde{a}_j|s_j) + \mathcal{H}_{target}))
        $$

        iv. **Update Alpha parameters** via gradient descent on $L_{\alpha}$

    4. **--- Soft Update Target Q-Networks ---**

        $$
        \phi'_1 \leftarrow \tau \phi_1 + (1-\tau) \phi'_1
        $$

        $$
        \phi'_2 \leftarrow \tau \phi_2 + (1-\tau) \phi'_2
        $$

    h. **IF** $d$ is `True` **THEN** **break**

    **END FOR**

**END FOR**

---

# PART 2: Key SAC Algorithm Features

## 1. How Many Neural Networks Does SAC Include?

In the code implementation, the SAC algorithm consists of **5 neural networks** and **1 learnable scalar parameter**:

1. **Policy Network (Actor)**: $\pi_{\theta}$
   * **Function**: Outputs action probability distributions given states
   * **Structure**: Outputs Gaussian distribution mean and log standard deviation

2. **First Q-Network (Critic 1)**: $Q_{\phi_1}$
   * **Function**: Estimates state-action value function
   * **Purpose**: Evaluates expected future rewards for state-action pairs with entropy regularization

3. **Second Q-Network (Critic 2)**: $Q_{\phi_2}$
   * **Function**: Same as Critic 1 but with independent parameters
   * **Purpose**: Mitigates Q-value overestimation via Clipped Double Q-Learning

4. **First Target Q-Network**: $Q_{\phi'_1}$
   * **Function**: Delayed copy of Critic 1 for stable target computation

5. **Second Target Q-Network**: $Q_{\phi'_2}$
   * **Function**: Delayed copy of Critic 2 for stable target computation

6. **Learnable Temperature Parameter** ($\alpha$):
   * Controls the importance of policy entropy in the objective
   * Automatically tuned during training via gradient descent

## 2. Loss Functions for All Networks

### A. Critic Network Loss

The Critic networks learn to estimate soft Q-values using a modified Bellman equation:

**Target Value Computation:**

$$
y = r + \gamma (1-d) \left( \min_{i=1,2} Q_{\phi'_i}(s', a') - \alpha \log\pi_{\theta}(a'|s') \right) \quad \text{where } a' \sim \pi_{\theta}(\cdot|s')
$$

**Critic Loss (MSE):**

$$
L(\phi_1, \phi_2) = \mathbb{E}_{(s,a,r,s',d)\sim\mathcal{D}} \left[ (Q_{\phi_1}(s,a) - y)^2 + (Q_{\phi_2}(s,a) - y)^2 \right]
$$

### B. Actor Network Loss

The Actor maximizes soft Q-values:

**Objective (to maximize):**

$$
J(\theta) = \mathbb{E}_{s\sim\mathcal{D}, \tilde{a}\sim\pi_{\theta}} \left[ \min_{i=1,2} Q_{\phi_i}(s, \tilde{a}) - \alpha \log\pi_{\theta}(\tilde{a}|s) \right]
$$

**Actor Loss (to minimize):**

$$
L(\theta) = \mathbb{E}_{s\sim\mathcal{D}, \tilde{a}\sim\pi_{\theta}} \left[ \alpha \log\pi_{\theta}(\tilde{a}|s) - \min_{i=1,2} Q_{\phi_i}(s, \tilde{a}) \right]
$$

### C. Temperature Parameter Loss

Automatic entropy tuning maintains policy entropy at target level:

$$
L(\alpha) = \mathbb{E}_{s\sim\mathcal{D}, \tilde{a}\sim\pi_{\theta}} \left[ -\alpha (\log\pi_{\theta}(\tilde{a}|s) + \mathcal{H}_{target}) \right]
$$

---

# PART 3: Some Confusing Points About SAC

## 1. Question 1 - Why is SAC Off-Policy?

Why do we use buffer actions when computing current Q-values for critic network updates, but use current policy-sampled actions when computing target Q-values; while when updating the Actor network, we use current policy-sampled actions when computing current Q-values?

### 1.1 The Essential Reason for Action Selection in Critic Updates

#### 1.1.1 Definition of Action-Value Function - The Essential Definition of Action-Value Function $Q^\pi(s,a)$

$$
Q^\pi(s,a) = \mathbb{E}_{\pi} \left[ \sum_{t=0}^{\infty} \gamma^t r_t \mid s_0=s, a_0=a \right]
$$

Execute a **deterministic action a** (can be any action) in state s, then continue following policy œÄ from the next step onwards, calculating the expected cumulative reward of the entire process.

#### 1.1.2 Logic of Action Selection in Critic Updates

##### 1.1.2.1 Computing Current Q-values - Using Actions from Buffer

According to the Q-function definition, $Q^\pi(s,a)$ must be able to evaluate the value of any deterministic action a. The $(s,a,r,s')$ from the buffer provides actually executed deterministic actions a and their real consequences. The Critic's learning objective is to accurately evaluate the value of "executing this specific action a, then continuing according to policy œÄ".

##### 1.1.2.2 Computing Target Q-values - Using Current Policy Sampled Actions

According to the Bellman equation:

$$
Q^\pi(s,a) = r(s,a) + \gamma \mathbb{E}_{a' \sim \pi(\cdot|s')} [Q^\pi(s',a')]
$$

The first step executes the deterministic action a from the buffer, obtaining reward r and next state s'. From the second step onwards, according to the definition, this part of value must be obtained by **following the current policy $\pi$**, therefore $a' \sim \pi(\cdot|s')$ must be sampled from the current policy.

### 1.2 Actor Updates Must Use Current Policy Actions

Actor is not learning the Q-function, but optimizing the policy itself. Its objective is to maximize:

$$
J(\pi) = \mathbb{E}_{s \sim D, a \sim \pi(\cdot|s)} [Q(s,a) - \alpha \log \pi(a|s)]
$$

The actions here must be sampled from current policy œÄ because:

1. The optimization target is policy parameter œÜ itself
2. Gradient $\nabla_\phi J(\pi)$ needs to flow through policy-generated actions
3. Only current policy actions contain information about parameter œÜ

### 1.3 Summary
Critic learns the value of "first execute any deterministic action, then continue according to policy"; Actor optimizes the policy's own performance. This essential difference determines that they use actions from different sources.

The **fundamental reason** A2C cannot use experience replay lies in the **tightly coupled On-Policy collaboration mechanism** between Actor and Critic. Once Critic uses experience replay for Off-Policy learning, it becomes a **"lagged judge"**, and this lag directly "contaminates" Actor updates, forcing Actor to be responsible for a **"ghost policy"** that no longer exists.

#### Mathematical Foundation: Policy Gradient Theorem

The correctness of this formula relies on two key consistencies:

1. **Sampling Consistency**: The sampling distribution $\pi_{\theta}$ in expectation $\mathbb{E}$ must be the same as the policy distribution in the gradient term $\log \pi_{\theta}$.
2. **Evaluation Consistency**: The advantage function $A^{\pi_{\theta}}(s_t, a_t) = Q^{\pi_{\theta}}(s_t, a_t) - V^{\pi_{\theta}}(s_t)$ is completely dependent on the current policy $\pi_{\theta}$. It evaluates how good the current policy's choice of action $a_t$ is relative to the **same policy $\pi_{\theta}$**'s average performance, then adjusts the probability of the policy choosing this action based on this relative performance to complete the current policy update.

#### State Value Function Definition

1. **A2C Critic's Correct Responsibility**:
    In standard On-Policy A2C, the Critic network (with parameters $w$) has the sole objective of learning the state value function $V^{\pi_{\theta}}(s)$ of the **current policy $\pi_{\theta}$**. Its learning depends on data generated by the **current policy**:
    $$
    V_w(s) \text{ aims to fit } \mathbb{E}_{a \sim \pi_{\theta}(\cdot|s), s' \sim P} [r(s,a) + \gamma V_w(s')]
    $$

2. **When Critic Uses Experience Replay, Responsibility Shifts**:
    If Critic samples data `(s, a, r, s')` generated by **old policy $\pi_{\theta_{old}}$** from experience replay for updates, its TD Target `y = r + Œ≥V_w(s')` is actually an unbiased estimate of **$V^{\pi_{\theta_{old}}}(s)$**. Therefore, **Critic's convergence target becomes fitting the old policy's value function $V^{\pi_{\theta_{old}}}$**. This is also determined by the definition of V-value function (using current policy's actions in current state and continuing to use current policy's actions thereafter to accumulate rewards, so V-value function evaluates not just the current state s, but also the current policy). This definition determines that using old tuples results in a state value function for the old policy.

#### Final Symptom: Actor Responsible for "Ghost Policy" Behavior

This "lagged Critic" directly leads to mismatch:

1. **Actor ($\pi_{\theta}$)**:
    Is working hard to evolve forward, urgently needing to know its **current** average performance $V^{\pi_{\theta}}$ to compute the correct advantage function $A^{\pi_{\theta}}$ for guiding the next evolution step.

2. **Critic ($V_w$)**:
    Is buried in studying historical archives (experience replay), with its output $V_w(s)$ approaching the value function of the **past** policy $\pi_{\theta_{old}}$ that no longer exists.

3. Finally, when Actor computes its vital gradient signal, the advantage function estimate becomes:

    $$
    A_{biased} \approx r_t + \gamma V^{\pi_{\theta_{old}}}(s_{t+1}) - V^{\pi_{\theta_{old}}}(s_t)
    $$
    This $A_{biased}$ is actually evaluating the behavior quality of **old policy $\pi_{\theta_{old}}$**.

4. When Actor uses this **advantage signal based on old policy values** to update **new policy's log probabilities**, the core contradiction emerges:

    $$
    \nabla_{\theta} J(\theta) \approx \mathbb{E} \left[ \nabla_{\theta} \log \pi_{\theta}(a_t|s_t) \cdot A^{\pi_{\theta_{old}}}(s_t, a_t) \right]
    $$
    The current policy $\pi_{\theta}$'s update is driven by evaluation criteria belonging to the past policy $\pi_{\theta_{old}}$.

### 2.4 Conclusion

A2C cannot be Off-Policy, not merely because the abstract policy gradient theorem requires sampling consistency, but because in its specific Actor-Critic architecture, this inconsistency produces a **biased advantage signal that evaluates "ghost policy"** through the **"lagged Critic"** mechanism, completely destroying Actor's optimization process. Actor and Critic must work together as a **real-time, synchronized On-Policy team**.

## 3. SAC Reparameterization

### 3.1 Why SAC Needs Reparameterization

> **SAC needs reparameterization because its Actor loss is $\mathbb{E}_{a \sim \pi_\theta} [Q(s,a)]$, where gradients must flow back through random sampling operations to policy parameters $\theta$; while A2C doesn't need reparameterization because its gradient form is $\nabla_\theta \log \pi_\theta(a|s)$, where gradients don't depend on how action a is sampled, only on the probability density given a.**

### 3.2 Mathematical Essence: Fundamental Difference Between Two Gradient Computations

1. SAC Actor's objective function:

    $$
    J(\pi_\theta) = \mathbb{E}_{s \sim \mathcal{D}} \left[ \mathbb{E}_{a \sim \pi_\theta(\cdot|s)} [Q(s,a)] \right]
    $$

    Corresponding gradient:

    $$
    \nabla_\theta J(\pi_\theta) = \nabla_\theta \mathbb{E}_{s \sim \mathcal{D}} \left[ \mathbb{E}_{a \sim \pi_\theta(\cdot|s)} [Q(s,a)] \right]
    $$

    **Problem**: In the inner expectation $\mathbb{E}_{a \sim \pi_\theta(\cdot|s)} [Q(s,a)]$, action $a$ is randomly sampled from distribution $\pi_\theta(\cdot|s)$, and this sampling operation is non-differentiable in standard automatic differentiation frameworks!

2. A2C gradient (no reparameterization needed):
    A2C policy gradient:
    $$
    \nabla_\theta J(\pi_\theta) = \mathbb{E}_{a \sim \pi_\theta(\cdot|s)} \left[ \nabla_\theta \log \pi_\theta(a|s) \cdot A(s,a) \right]
    $$

    **Key**: Here the gradient is $\nabla_\theta \log \pi_\theta(a|s)$ ‚Äî it's a **deterministic function**! Given state s and action a, $\log \pi_\theta(a|s)$ is a differentiable function of parameters $\theta$, completely independent of how a was sampled.

3. The Difference:
    > **In SAC, we need to compute $\nabla_\theta Q(s, a(\theta))$, where $a(\theta)$ is a variable generated through a random process from $\pi_\theta$; while in A2C, we only need to compute $\nabla_\theta \log \pi_\theta(a)$, which is a deterministic function of $\theta$ given a.**
    The former requires gradients to flow through random sampling, while the latter doesn't.

### 3.3 Mathematical Principle of Reparameterization

#### 3.3.1 Original Sampling (Non-differentiable):

$$
a \sim \mathcal{N}(\mu_\theta(s), \sigma_\theta(s))
$$

Here, randomness directly acts on output a, preventing gradient backpropagation.

#### 3.3.2 Reparameterized (Differentiable):

$$
\epsilon \sim \mathcal{N}(0, I)
$$

$$
a = \mu_\theta(s) + \sigma_\theta(s) \cdot \epsilon
$$

**Key Changes**:
- Randomness moved to input side ($\epsilon$), independent of parameters $\theta$
- Output a becomes a deterministic function of parameters $\theta$
- Gradients can flow normally through chain rule: $\nabla_\theta a = \nabla_\theta \mu_\theta(s) + \nabla_\theta \sigma_\theta(s) \cdot \epsilon$

Thus, we can compute:

$$
\nabla_\theta Q(s, a) = \nabla_a Q(s, a) \cdot \nabla_\theta a
$$

### 3.4 Code Implementation Details (Based on This Project)

#### 3.4.1 Reparameterized Sampling (the `evaluate` function in code):

```python
def evaluate(self, state: torch.Tensor, deterministic: bool = False) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    mean, log_std = self.forward_actor(state)
    std = log_std.exp()
    normal = torch.distributions.Normal(mean, std)

    if deterministic:
        action_raw = mean
    else:
        # --- Reparameterization Trick ---
        # x_t ~ N(mean, std)
        action_raw = mean + std * torch.randn_like(std)  # Key: Reparameterization!
    
    action_unscaled = torch.tanh(action_raw)
    action_scaled = action_unscaled * self.action_scale + self.action_bias

    # Calculate log_prob
    log_prob = normal.log_prob(action_raw) - torch.log(1 - action_unscaled.pow(2) + 1e-6)
    log_prob = log_prob.sum(dim=-1, keepdim=True)

    return action_scaled, log_prob, action_unscaled
```

Key lines:
1. `action_raw = mean + std * torch.randn_like(std)`
2. `torch.randn_like(std)` generates parameter-independent random noise $\epsilon$
3. `mean` and `std` are network outputs, dependent on parameters $\theta$
4. Thus, `action_raw` becomes a differentiable function of $\theta$

#### 3.4.2 SAC Actor Update (Requires Reparameterization):

```python
# Sample from current policy (requires gradients)
pi_actions, log_probs, _ = self.network.evaluate(states)  # Reparameterized sampling!

# Calculate Q-values
Q1_pi = self.network.critic_1(torch.cat([states, pi_actions], dim=-1))
Q2_pi = self.network.critic_2(torch.cat([states, pi_actions], dim=-1))
min_Q_pi = torch.min(Q1_pi, Q2_pi)

# Actor loss
actor_loss = (self.alpha.detach() * log_probs - min_Q_pi).mean()

# Backpropagation
self.actor_optimizer.zero_grad()
actor_loss.backward()  # Gradients can flow back through pi_actions to policy network!
self.actor_optimizer.step()
```

If reparameterization wasn't used, pi_actions would be a non-differentiable random variable, causing actor_loss.backward() to fail at pi_actions, preventing policy network parameter updates.

### 3.5 Why A2C Doesn't Need Reparameterization?

A2C Actor update code (simplified version):

```python
# Assume this is A2C update
states, actions, advantages = sample_from_buffer()

# Calculate log_prob
log_prob = current_policy.log_prob(states, actions)  # Given s and a, calculate probability density

# Actor loss
actor_loss = -(log_prob * advantages).mean()

# Backpropagation
actor_loss.backward()  # Gradients only flow through log_prob, independent of how a was sampled!
```

Key points:

1. log_prob = log œÄ_Œ∏(a|s) is a deterministic function ‚Äî given s and a, it's a differentiable function of Œ∏
2. Gradient computation doesn't depend on how action a was sampled
3. Even if a is sampled from old policy, as long as we can compute log œÄ_Œ∏(a|s), we get correct gradients

---

## üöÜ Train Environment (TrainEnv)

The `HighSpeedTrainEnv` class implements a comprehensive high-speed train simulation environment following the Gymnasium API standards.

### Environment Overview

The environment simulates high-speed train operation based on space-domain differential equations:

1. **Energy Evolution**: `dE/ds = (u(s) - W(s)) / (m * (1 + gamma))`
2. **Time Evolution**: `dt/ds = 1 / v(s)`

Where:
- `E`: Kinetic energy per unit mass
- `u(s)`: Control force (traction/braking)
- `W(s)`: Total resistance
- `m`: Train mass
- `gamma`: Rotational mass coefficient
- `v(s)`: Train speed

### Key Features

#### 1. Physical Dynamics
- **Realistic train physics** with mass, inertia, and rotational effects
- **Complex resistance model** including basic resistance, gradient resistance, and curve resistance
- **Force characteristics** from real train traction and braking curves
- **Speed-dependent limitations** based on train capabilities

#### 2. Railway Infrastructure
- **Line gradients** affecting train performance
- **Speed limits** varying along the route
- **Curve radius** influencing resistance
- **Electric phase separation** zones with power restrictions
- **Station locations** for operational planning

#### 3. Observation Space (8 dimensions)

The environment provides normalized observations:

```python
obs[0] = current_speed / 350.0           # Current speed (normalized)
obs[1] = current_position_progress        # Position progress [0-1]
obs[2] = remaining_distance_ratio         # Remaining distance [1-0]
obs[3] = current_time_progress           # Time progress [0-1]
obs[4] = remaining_time_ratio            # Remaining time [1-0]
obs[5] = speed_limit / 350.0             # Current speed limit (normalized)
obs[6] = target_speed / 350.0            # Target speed (normalized)
obs[7] = time_pressure                   # Time pressure [-1,1]
```

#### 4. Action Space

- **Continuous control**: Action ‚àà [-1, 1]
- **Intelligent mapping**: Automatically converts to appropriate traction/braking forces
- **Safety constraints**: Built-in speed limit and endpoint constraints
- **Smooth operation**: Encourages gradual force changes

#### 5. Reward System

The reward function balances multiple objectives:

```python
# Base progress reward
reward += 5.0  # Forward movement encouragement

# Speed efficiency reward
speed_efficiency = min(current_v / optimal_speed, 1.0)
reward += 3.0 * speed_efficiency

# Time management
if abs(time_diff) > 300:  # 5-minute tolerance
    time_pressure_reward = calculate_time_pressure(time_diff)
    reward += time_pressure_reward

# Terminal rewards (completion)
if is_terminated:
    completion_reward = 100.0
    punctuality_reward = calculate_punctuality_bonus(time_error)
    speed_penalty = calculate_final_speed_penalty(final_speed)
    reward += completion_reward + punctuality_reward - speed_penalty
```

### Environment Initialization

```python
env = HighSpeedTrainEnv(
    train_params_path="train_characteristics.xlsx",
    line_params_path="railway_line_data.xlsx",
    delta_step_length_m=1000,        # Simulation step size (meters)
    target_time_s=12000,             # Target journey time (seconds)
    start_s_m=1225000,               # Start position (meters)
    end_s_m=1638950,                 # End position (meters)
    start_v_mps=1.0,                 # Initial speed (m/s)
    punctuality_tolerance_s=60.0     # Punctuality tolerance (seconds)
)
```

### Performance Optimizations

The environment is highly optimized for training efficiency:

#### 1. Numba JIT Compilation
```python
@njit(cache=True)
def fast_step_dynamics_numba(current_v, current_E, current_s, control_force, ...):
    # Ultra-fast dynamics computation
    # 10-50x speedup over pure Python
```

#### 2. Precomputed Parameters
- Force characteristics converted to numpy arrays
- Line data optimized for fast access
- Resistance coefficients cached
- Distance grids pre-generated

#### 3. Vectorized Operations
- Batch resistance calculations
- Parallel force interpolations
- Efficient memory management

### Integration with RL Algorithms

The environment seamlessly integrates with popular RL libraries:

```python
# SAC Training Example
from TrainControl import SACAgent

agent = SACAgent(env=env, lr=1e-4, gamma=0.99)
agent.train(max_episodes=10000, max_steps=max_steps)

# A2C Training Example  
from RL_A2C import A2CAgent

agent = A2CAgent(env=env, lr=3e-4, gamma=0.99)
agent.entire_train()
```

## üìà Results and Performance

### Training Metrics
- **Convergence**: Typically converges within 5000-8000 episodes
- **Performance**: Achieves 95%+ punctuality with smooth operation
- **Efficiency**: 10-50x faster than traditional simulation environments

### Real-world Validation
- Physics model validated against real high-speed train data
- Force characteristics based on actual train specifications
- Line data from operational high-speed railways

## üï∞Ô∏è Contributing

Contributions are welcome! Please feel free to submit a Pull Request. For major changes, please open an issue first to discuss what you would like to change.

## üìù License

This project is open source and available under the [MIT License](LICENSE).

---

**Note**: This implementation provides a realistic and efficient simulation environment for high-speed train control research. The combination of accurate physics modeling, intelligent action mapping, and performance optimizations makes it suitable for both research and practical applications in railway operation optimization.
